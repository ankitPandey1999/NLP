{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ngram_range to various parameters like (1, 3) would build\n",
    "# feature vectors consisting of all unigrams, bigrams, and trigrams.\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) # minimum frequency of 1 \n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print (new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "# Function to display features as a data frame\n",
    "import pandas as pd\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print (df)\n",
    "\n",
    "display_features(features, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words model is good, but the vectors are completely based on absolute\n",
    "frequencies of word occurrences. This has some potential problems where words that\n",
    "may tend to occur a lot across all documents in the corpus will have higher frequencies\n",
    "and will tend to overshadow other words that may not occur as frequently but may\n",
    "be more interesting and effective as features to identify specific categories for the\n",
    "documents. This is where TF-IDF comes into the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "import numpy as np\n",
    "#from feature_extractors import tfidf_transformer\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "# build tfidf transformer and show train corpus tfidf features\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Maths behind TDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "# show term frequencies\n",
    "display_features(tf, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later\n",
    "\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "display_features([np.round(idf, 2)], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n"
     ]
    }
   ],
   "source": [
    "# compute idf diagonal matrix\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "print (np.round(idf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "# show tfidf feature matrix\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Normal form [2.5  4.35 3.5  2.89]\n"
     ]
    }
   ],
   "source": [
    "# compute L2 norms\n",
    "norms = norm(tfidf, axis=1)\n",
    "# print norms for each document\n",
    "print(\"L2 Normal form\",np.round(norms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "# show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new doc term freqs from bow freqs\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf using idf matrix from train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "# show new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# implement a generic function that can directly compute the tfidf-based feature vectors for documents from the\n",
    "#raw documents themselves.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "# build tfidf vectorizer and get training corpus feature vectors\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# get tfidf feature vector for the new document\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec\n",
    "framework is much faster than other neural network–based implementations and does\n",
    "not require manual labels to create meaningful representations among words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Internally, it constructs a vocabulary based on the input text\n",
    "documents and learns vector representations for words based on various techniques\n",
    "mentioned earlier, and once this is complete, it builds a model that can be used to\n",
    "extract word vectors for each word in a document. Using various techniques like average\n",
    "weighting or tfidf weighting, we can compute the averaged vector representation of a\n",
    "document using its word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': ['blue', 'beautiful', 'is', 'the'],\n",
       " 'blue': ['sky', 'is', 'beautiful', 'the']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in new_doc]       \n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 10    # no of Word vector dimensions  \n",
    "window_context = 10  # length of the window of words that should be considered for the algorithm                                                                                    \n",
    "min_word_count = 2   # Minimum word count for the word to be considered. Helps in removing\n",
    "                     # very specific words because they occur very rarely in the documents                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "\n",
    "model = word2vec.Word2Vec(TOKENIZED_CORPUS, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample)\n",
    "\n",
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['sky', 'blue']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.023  0.021  0.004  0.004  0.02  -0.02  -0.007  0.005  0.018 -0.008]\n",
      " [-0.012  0.025  0.008 -0.001  0.025 -0.014  0.004  0.022  0.04  -0.011]\n",
      " [-0.024  0.023  0.01   0.004  0.014 -0.011 -0.002  0.014  0.023 -0.008]\n",
      " [-0.046  0.026 -0.034 -0.041  0.026 -0.028 -0.036  0.031  0.035  0.012]]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import averaged_word_vectorizer\n",
    "\n",
    "\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                 model=model,\n",
    "                                                 num_features=10)\n",
    "print (np.round(avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.001  0.019 -0.025 -0.018  0.037 -0.027 -0.016  0.038  0.035 -0.005]]\n"
     ]
    }
   ],
   "source": [
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                    model=model,\n",
    "                                                    num_features=10)\n",
    "print (np.round(nd_avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.023  0.02   0.007  0.008  0.018 -0.02  -0.007  0.001  0.014 -0.009]\n",
      " [-0.007  0.024  0.011  0.003  0.027 -0.015  0.008  0.019  0.04  -0.014]\n",
      " [-0.024  0.023  0.014  0.008  0.012 -0.009 -0.001  0.013  0.02  -0.008]\n",
      " [-0.046  0.026 -0.034 -0.041  0.026 -0.028 -0.036  0.031  0.035  0.012]]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print (np.round(wt_tfidf_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.003  0.018 -0.024 -0.015  0.038 -0.027 -0.014  0.039  0.034 -0.007]]\n"
     ]
    }
   ],
   "source": [
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                     tfidf_vectors=nd_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print (np.round(nd_wt_tfidf_word_vec_features, 3) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
