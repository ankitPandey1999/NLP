{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ngram_range to various parameters like (1, 3) would build\n",
    "# feature vectors consisting of all unigrams, bigrams, and trigrams.\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) # minimum frequency of 1 \n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print (new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "# Function to display features as a data frame\n",
    "import pandas as pd\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print (df)\n",
    "\n",
    "display_features(features, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words model is good, but the vectors are completely based on absolute\n",
    "frequencies of word occurrences. This has some potential problems where words that\n",
    "may tend to occur a lot across all documents in the corpus will have higher frequencies\n",
    "and will tend to overshadow other words that may not occur as frequently but may\n",
    "be more interesting and effective as features to identify specific categories for the\n",
    "documents. This is where TF-IDF comes into the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "import numpy as np\n",
    "#from feature_extractors import tfidf_transformer\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "# build tfidf transformer and show train corpus tfidf features\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Maths behind TDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "# show term frequencies\n",
    "display_features(tf, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later\n",
    "\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "display_features([np.round(idf, 2)], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n"
     ]
    }
   ],
   "source": [
    "# compute idf diagonal matrix\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "print (np.round(idf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "# show tfidf feature matrix\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Normal form [2.5  4.35 3.5  2.89]\n"
     ]
    }
   ],
   "source": [
    "# compute L2 norms\n",
    "norms = norm(tfidf, axis=1)\n",
    "# print norms for each document\n",
    "print(\"L2 Normal form\",np.round(norms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "# show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new doc term freqs from bow freqs\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf using idf matrix from train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "# show new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# implement a generic function that can directly compute the tfidf-based feature vectors for documents from the\n",
    "#raw documents themselves.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "# build tfidf vectorizer and get training corpus feature vectors\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# get tfidf feature vector for the new document\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
